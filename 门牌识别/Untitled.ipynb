{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定长CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1f7849304b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m  \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m  \u001b[0malbumentations\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from  PIL import Image\n",
    "import cv2\n",
    "import  albumentations as al\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                        ])\n",
    "def al_trans(img):\n",
    "    al_trans = al.Compose([\n",
    "            al.Resize(height=64,width=192),\n",
    "            al.OneOf([\n",
    "                al.IAAAdditiveGaussianNoise(),\n",
    "                al.GaussNoise(),\n",
    "            ], p=0.6),\n",
    "            al.OneOf([\n",
    "                al.MotionBlur(p=.2),\n",
    "                al.MedianBlur(blur_limit=3, p=.1),\n",
    "                al.Blur(blur_limit=3, p=.1),\n",
    "            ], p=0.2),\n",
    "            al.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=.2),\n",
    "            al.RandomBrightnessContrast(p=.3),\n",
    "            al.OneOf([\n",
    "                al.OpticalDistortion(p=0.3),\n",
    "                al.GridDistortion(p=.1),\n",
    "                al.IAAPiecewiseAffine(p=0.3),\n",
    "            ], p=0.2),\n",
    "            al.OneOf([\n",
    "                al.CLAHE(clip_limit=2),\n",
    "                al.IAASharpen(),\n",
    "                al.IAAEmboss(),\n",
    "            ], p=0.3),\n",
    "\n",
    "    ])\n",
    "    img = al_trans(image=img)['image']\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    img = trans(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSet(nn.Module):\n",
    "    def __init__(self, model_cnn='resnet18',use_spp=False): #spp只添加在了efficientnet（感觉有点问题，需要探索一下）\n",
    "        super(ModelSet, self).__init__()\n",
    "        self.spp = SPPLayer(3)\n",
    "        self.use_spp = use_spp\n",
    "        self.model_cnn = model_cnn\n",
    "        if self.model_cnn[:6] == 'resnet':\n",
    "            model_conv = self.cnn_resnet()\n",
    "            in_feature = list(model_conv.children())[-1].in_features\n",
    "            model_conv.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "            model_conv = nn.Sequential(*list(model_conv.children())[:-1])\n",
    "\n",
    "\n",
    "        if self.model_cnn[:-3] == 'efficientnet':\n",
    "            model_conv = EfficientNet.from_pretrained(self.model_cnn)\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "            in_feature = list(model_conv.children())[-5].num_features\n",
    "        if self.model_cnn == 'Xception':\n",
    "            model_conv = XceptionNet.xception()\n",
    "            model_conv = nn.Sequential(*list(model_conv.children())[:-1])\n",
    "            model_conv.add_module('AdaptiveAvgPool2d', nn.AdaptiveAvgPool2d(1))\n",
    "            in_feature = list(model_conv.children())[-2].num_features\n",
    "        self.spp_layer = SPPLayer(num_levels=3)\n",
    "        self.cnn = model_conv\n",
    "        if self.use_spp == True:\n",
    "            in_feature = calc_auto(3, in_feature*4)\n",
    "        self.fc1 = nn.Linear(in_feature, 11)\n",
    "        self.fc2 = nn.Linear(in_feature, 11)\n",
    "        self.fc3 = nn.Linear(in_feature, 11)\n",
    "        self.fc4 = nn.Linear(in_feature, 11)\n",
    "        self.fc5 = nn.Linear(in_feature, 11)\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "\n",
    "        if self.model_cnn[:-3] == 'efficientnet':\n",
    "            feat = self.cnn.extract_features(img)\n",
    "            if self.use_spp == True:\n",
    "                feat = self.spp(feat)\n",
    "            else:\n",
    "                feat = self.avgpool(feat)\n",
    "        else:\n",
    "            feat = self.cnn(img)\n",
    "        feat = feat.view(feat.shape[0], -1)\n",
    "        c1 = self.fc1(feat)\n",
    "        c2 = self.fc2(feat)\n",
    "        c3 = self.fc3(feat)\n",
    "        c4 = self.fc4(feat)\n",
    "        c5 = self.fc5(feat)\n",
    "\n",
    "        return c1, c2, c3, c4, c5\n",
    "\n",
    "    def cnn_resnet(self):\n",
    "        if self.model_cnn == 'resnet18':\n",
    "            return models.resnet18(pretrained=True)\n",
    "        if self.model_cnn == 'resnet34':\n",
    "            return models.resnet34(pretrained=True)\n",
    "        if self.model_cnn == 'resnet50':\n",
    "            return models.resnet50(pretrained=True)\n",
    "        if self.model_cnn == 'resnet101':\n",
    "            return models.resnet101(pretrained=True)\n",
    "        if self.model_cnn == 'resnet152':\n",
    "            return models.resnet152(pretrained=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  model = ModelSet(model_cnn='efficientnet-b0',use_spp=True)\n",
    "  x = torch.rand((2,3,32,32))\n",
    "  r = model(x)\n",
    "  print(r[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPPLayer(torch.nn.Module):\n",
    "    def __init__(self, num_levels, pool_type='max_pool'):\n",
    "        super(SPPLayer, self).__init__()\n",
    "        self.num_levels = num_levels\n",
    "        self.pool_type = pool_type\n",
    "    def forward(self, x):\n",
    "        num, c, h, w = x.size()\n",
    "        level = 1\n",
    "        for i in range(self.num_levels):\n",
    "            level <<= 1\n",
    "            kernel_size = (math.ceil(h / level), math.ceil(w / level))  # kernel_size = (h, w)\n",
    "            padding = (\n",
    "                math.floor((kernel_size[0] * level - h + 1) / 2), math.floor((kernel_size[1] * level - w + 1) / 2))\n",
    "            zero_pad = torch.nn.ZeroPad2d((padding[1], padding[1], padding[0], padding[0]))\n",
    "            x_new = zero_pad(x)\n",
    "            h_new, w_new = x_new.size()[2:]\n",
    "            kernel_size = (math.ceil(h_new / level), math.ceil(w_new / level))\n",
    "            stride = (math.floor(h_new / level), math.floor(w_new / level))\n",
    "            if self.pool_type == 'max_pool':\n",
    "                tensor = F.max_pool2d(x_new, kernel_size=kernel_size, stride=stride).view(num, -1)\n",
    "            elif self.pool_type == 'avg_pool':\n",
    "                tensor = F.avg_pool2d(x_new, kernel_size=kernel_size, stride=stride).view(num, -1)\n",
    "            if (i == 0):\n",
    "                x_flatten = tensor.view(num, -1)\n",
    "            else:\n",
    "                x_flatten = torch.cat((x_flatten, tensor.view(num, -1)), 1)\n",
    "\n",
    "        return x_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay = 1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方式1 ： data1[i] !=data2[i] data2[i]==data3[i]==data4[i]\n",
    "     \n",
    "# 方式2 ： data1[i] !=data2[i] data2[i]==data3[i] ||  data2[i]==data4[i]\n",
    "data1 =pd.read_csv(r'D:\\r1.csv') \n",
    "data1 =pd.read_csv(r'D:\\r2.csv') \n",
    "sum=0\n",
    "for i in range(len(data1)):\n",
    "        if (str(data1.loc[i,'file_code']) !=str(data2.loc[i,'file_code'])) and (len(str(data1.loc[i,'file_code'])))<(len(str(data1.loc[i,'file_code']))) :\n",
    "            print(data1.loc[i,'file_code'],data2.loc[i,'file_code'])\n",
    "            data1.loc[i,'file_code'] = data2.loc[i,'file_code']\n",
    "            sum+=1\n",
    "        \n",
    "print(sum)\n",
    "\n",
    "data1 =pd.read_csv(r'D:\\r1.csv') \n",
    "data1 =pd.read_csv(r'D:\\r2.csv') \n",
    "data2 =pd.read_csv(r'D:\\r3.csv')\n",
    "data3 =pd.read_csv(r'D:\\r4.csv')\n",
    "data4 =pd.read_csv(r'D:\\r5.csv')\n",
    "sum=0\n",
    "for i in range(len(data1)):\n",
    "        if (str(data1.loc[i,'file_code']) !=str(data2.loc[i,'file_code']))&& (str(data2.loc[i,'file_code'])==str(data3.loc[i,'file_code'])==str(data4.loc[i,'file_code']) )  :\n",
    "            print(data1.loc[i,'file_code'],data2.loc[i,'file_code'])\n",
    "            data1.loc[i,'file_code'] = data2.loc[i,'file_code']\n",
    "            sum+=1\n",
    "        \n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 转换成符合输入数据格式\n",
    "import json\n",
    "import os, sys, glob, shutil, json\n",
    "import  matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from lxml.etree import Element, SubElement, tostring\n",
    "import pprint\n",
    "from xml.dom.minidom import parseString\n",
    "from tqdm import tqdm\n",
    "from xml.etree import ElementTree as ET \n",
    "\n",
    "\n",
    "def xml_make(train_imgname,train_label,train_left,train_top,train_width,train_height):\n",
    "    node_root = Element('annotation')\n",
    "\n",
    "    node_filename = SubElement(node_root, 'filename')\n",
    "    node_filename.text = str(train_imgname)\n",
    "    img = Image.open(\"mnt/mchar_train/\"+str(train_imgname)).convert('RGB')\n",
    "    #img = Image.open(\"mnt/mchar_val/\"+str(train_imgname)).convert('RGB')\n",
    "    width=img.width\n",
    "    height=img.height\n",
    "    node_size = SubElement(node_root, 'size')\n",
    "    node_width = SubElement(node_size, 'width')\n",
    "    node_width.text = str(width)\n",
    "\n",
    "    node_height = SubElement(node_size, 'height')\n",
    "    node_height.text = str(height)\n",
    "\n",
    "    node_depth = SubElement(node_size, 'depth')\n",
    "    node_depth.text = '3'\n",
    "\n",
    "    \n",
    "    for i in range(len(train_left)):\n",
    "        node_object = SubElement(node_root, 'object')\n",
    "        node_name = SubElement(node_object, 'name')\n",
    "        node_name.text = str(train_label[i])\n",
    "        node_difficult = SubElement(node_object, 'difficult')\n",
    "        node_difficult.text = '0'\n",
    "        node_bndbox = SubElement(node_object, 'bndbox')\n",
    "        node_xmin = SubElement(node_bndbox, 'xmin')\n",
    "        node_xmin.text = str(int(train_left[i]))\n",
    "        node_ymin = SubElement(node_bndbox, 'ymin')\n",
    "        node_ymin.text = str(int(train_top[i]))\n",
    "        node_xmax = SubElement(node_bndbox, 'xmax')\n",
    "        node_xmax.text = str(int(train_left[i]+train_width[i]))\n",
    "        node_ymax = SubElement(node_bndbox, 'ymax')\n",
    "        node_ymax.text = str(int(train_top[i]+train_height[i]))\n",
    "    if not os.path.exists('/mnt/train_xml/'):\n",
    "         os.makedirs('/mnt/train_xml/')\n",
    "    savename=os.path.join(\"/mnt/train_xml/\", train_imgname.split('.')[0]+'.xml')     \n",
    "#     if not os.path.exists('mnt/val_xml/'):\n",
    "#          os.makedirs('mnt/val_xml/')\n",
    "#     savename=os.path.join(\"mnt/val_xml/\", train_imgname.split('.')[0]+'.xml')     \n",
    "    tree = ET.ElementTree(node_root)\n",
    "    tree.write(savename)\n",
    "    \n",
    "train_json = json.load(open(r'content/drive/My Drive/mchar_train.json'))\n",
    "train_imgname=list(train_json.keys())\n",
    "train_label = [train_json[x]['label'] for x in train_json]\n",
    "train_height = [train_json[x]['height'] for x in train_json]\n",
    "train_left = [train_json[x]['left'] for x in train_json]\n",
    "train_top = [train_json[x]['top'] for x in train_json]\n",
    "train_width = [train_json[x]['width'] for x in train_json]\n",
    "\n",
    "for i in tqdm(range(len(train_imgname))):\n",
    "      xml_make(train_imgname[i],train_label[i],train_left[i],train_top[i],train_width[i],train_height[i])\n",
    "        \n",
    "val_json = json.load(open(r'content/drive/My Drive/mchar_val.json'))\n",
    "val_imgname=list(val_json.keys())\n",
    "val_label = [val_json[x]['label'] for x in val_json]\n",
    "val_height = [val_json[x]['height'] for x in val_json]\n",
    "val_left = [val_json[x]['left'] for x in val_json]\n",
    "val_top = [val_json[x]['top'] for x in val_json]\n",
    "val_width = [val_json[x]['width'] for x in val_json]\n",
    "\n",
    "for i in tqdm(range(len(val_imgname))):\n",
    "    xml_make(val_imgname[i],val_label[i],val_left[i],val_top[i],val_width[i],val_height[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "random.seed(912)\n",
    "trainval_percent = 1\n",
    "train_percent = 1\n",
    "\n",
    "## 训练集 / 验证集  需要注意转换完保存\n",
    "## 或者将训练集和验证集的xml 合并为一个文件夹，再按0.8 0.2 ,增加原本训练集的数量\n",
    "# xmlfilepath = '/mnt/all_xml/' \n",
    "xmlfilepath = '/mnt/train_xml/' \n",
    "# xmlfilepath = '/mnt/val_xml/'\n",
    "total_xml = os.listdir(xmlfilepath)\n",
    "num = len(total_xml)\n",
    "list = range(num)\n",
    "tv = int(num * trainval_percent)\n",
    "tr = int(tv * train_percent)\n",
    "trainval = random.sample(list, tv)\n",
    "train = random.sample(trainval, tr)\n",
    "if not os.path.exists('/mnt/ImgSet/'):\n",
    "         os.makedirs('/mnt/ImgSet/')\n",
    "ftrainval = open('/mnt/ImgSet/trainval.txt', 'w')\n",
    "ftest = open('/mnt/ImgSet/test.txt', 'w')\n",
    "ftrain = open('/mnt/ImgSet/train.txt', 'w')\n",
    "fval = open('/mnt/ImgSet/val.txt', 'w')\n",
    "\n",
    "for i in list:\n",
    "    name = total_xml[i][:-4] + '\\n'\n",
    "    if i in trainval:\n",
    "        ftrainval.write(name)\n",
    "        if i in train:\n",
    "            ftest.write(name)\n",
    "        else:\n",
    "            fval.write(name)\n",
    "    else:\n",
    "        ftrain.write(name)\n",
    "\n",
    "ftrainval.close()\n",
    "ftrain.close()\n",
    "fval.close()\n",
    "ftest.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "import os\n",
    "from os import listdir, getcwd\n",
    "from os.path import join\n",
    "\n",
    "sets = ['val']\n",
    "# sets = ['train']\n",
    "classes = ['1','2','3','4','5','6','7','8','9','0']\n",
    "\n",
    "\n",
    "def convert(size, box):\n",
    "    dw = 1. / size[0]\n",
    "    dh = 1. / size[1]\n",
    "    x = (box[0] + box[1]) / 2.0\n",
    "    y = (box[2] + box[3]) / 2.0\n",
    "    w = box[1] - box[0]\n",
    "    h = box[3] - box[2]\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    if x<0:\n",
    "        x=0\n",
    "        print(image_id)\n",
    "    elif x>1:\n",
    "        x=1\n",
    "        print(image_id)\n",
    "    if w<0:\n",
    "        w=0\n",
    "        print(image_id)\n",
    "    elif w>1:\n",
    "        w=1\n",
    "        print(image_id)\n",
    "    if y<0:\n",
    "        y=0\n",
    "        print(image_id)\n",
    "    elif y>1:\n",
    "        y=1\n",
    "        print(image_id)\n",
    "    if h<0:\n",
    "        h=0\n",
    "        print(image_id)\n",
    "    elif h>1:\n",
    "        h=1 \n",
    "        print(image_id)\n",
    "    return (x, y, w, h)\n",
    "    \n",
    "\n",
    "\n",
    "def convert_annotation(image_id):\n",
    "    # in_file = open('/mnt/train_xml/%s.xml' % (image_id))\n",
    "    # out_file = open('/mnt/train_labels/%s.txt' % (image_id), 'w')\n",
    "    in_file = open('/mnt/val_xml/%s.xml' % (image_id))\n",
    "    out_file = open('/mnt/val_labels/%s.txt' % (image_id), 'w')\n",
    "    tree = ET.parse(in_file)\n",
    "    root = tree.getroot()\n",
    "    size = root.find('size')\n",
    "    w = int(size.find('width').text)\n",
    "    h = int(size.find('height').text)\n",
    "    for obj in root.iter('object'):\n",
    "        difficult = obj.find('difficult').text\n",
    "        cls = obj.find('name').text\n",
    "        if cls not in classes or int(difficult)==1:\n",
    "            continue\n",
    "        cls_id = classes.index(cls)\n",
    "        xmlbox = obj.find('bndbox')\n",
    "        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))\n",
    "        # print('(w,h)',w,h,'b',b)\n",
    "        bb = convert((w,h), b)\n",
    "        out_file.write(str(cls_id) + \" \" + \" \".join([str(a) for a in bb]) + '\\n')\n",
    "\n",
    "\n",
    "wd = getcwd()\n",
    "for image_set in sets:\n",
    "    # if not os.path.exists('/mnt/train_labels/'):\n",
    "    #     os.makedirs('/mnt/train_labels/')\n",
    "    if not os.path.exists('mnt/val_labels/'):\n",
    "         os.makedirs('mnt/val_labels/')\n",
    "    image_ids = open('/content/drive/My Drive/TianchiCV/Image_Set/%s.txt' % (image_set)).read().strip().split()\n",
    "    list_file = open('mnt/%s.txt' % (image_set), 'w')\n",
    "    for image_id in tqdm(image_ids):\n",
    "        list_file.write('mnt/mchar_val/%06s.png\\n' % (image_id))\n",
    "        # list_file.write('/mnt/mchar_train/%06s.png\\n' % (image_id))\n",
    "        convert_annotation(image_id)\n",
    "    list_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##安装所需环境\n",
    "!pip install -U -r /content/drive/My\\ Drive/yolov5-master/yolov5-master/requirements.txt\n",
    "## 开启multi-scale  输入尺度320 \n",
    "!cd /content/drive/My\\ Drive/yolov5-master/yolov5-master && python train.py  --batch-size 16  --cache-images #--multi-scale\n",
    "#只输出结果的txt,没有输出图像(可在detect.py 130 处自行注释语句)\n",
    "!cd /content/drive/My\\ Drive/yolov5-master/yolov5-master && python detect.py  --weights 模型位置  --output 输出位置--save-txt --img-size 192 \\ ## 192 第一次infer 0.93+   --iou-thres 值为 0.35\n",
    "#--augment 使用数据增强时建议提高阈值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 将txt中的结果按识别出的字符按从左到右的顺序排序，保存到提交文件\n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "file_path=r'mnt/r//'\n",
    "txt_file = sorted(os.listdir(file_path))\n",
    "result = pd.read_csv(r'/content/drive/My Drive/mchar_sample_submit_A.csv')\n",
    "for i,name in  tqdm(enumerate(txt_file)):\n",
    "    if name[-3:]=='txt':\n",
    "    txt = np.loadtxt(file_path+txt_file[i])\n",
    "    if txt.ndim!=1:\n",
    "        idex=np.lexsort([txt[:,1]])\n",
    "        txt = txt[idex, :]\n",
    "        t=''\n",
    "        for k in range(len(txt)):\n",
    "            if  np.int(txt[k][0]) == 9:\n",
    "                txt[k][0]=-1\n",
    "            t=t+(str(np.int(txt[k][0])+1))\n",
    "#         print(i,name,t)\n",
    "        result.loc[result.file_name==name[:-3]+'png','file_code']=t\n",
    "    else:\n",
    "        t=''\n",
    "        if  np.int(txt[0]) == 9:\n",
    "                txt[0]=-1\n",
    "        t=t+(str(np.int(txt[0])+1))\n",
    "#         print(i,name,t)\n",
    "        result.loc[result.file_name==name[:-3]+'png','file_code']=t\n",
    "result.to_csv(r'/content/drive/My Drive/YOLO5_result.csv',index=None)\n",
    "result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
